{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"Sample.pdf\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting/Chunking the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 100,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")\n",
    "\n",
    "texts = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pinecone Client and the OpenAI embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key= os.environ['OPENAI_API_KEY'])\n",
    "pc = Pinecone(api_key = os.environ['PINECONE_API_KEY'])\n",
    "\n",
    "# Check if index exist\n",
    "index = pc.Index(\"ragchain-db\")\n",
    "\n",
    "#Delete all Vectors on the index\n",
    "# index.delete(delete_all=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert embeddings to pinecone and set a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone as langpc\n",
    "vectordb = langpc.from_documents(texts, embeddings, index_name = \"ragchain-db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='As an AI language model, I do not have access to personal information about individuals unless it has been shared with me in the course of our conversation. I can provide general information about skills commonly associated with the name Lorna Alvarado. If you are referring to a specific individual, please provide more information.')"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "llm.invoke(\"What skills do Lorna Alvarado have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Memory and Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages= True)\n",
    "chain = ConversationalRetrievalChain.from_llm(llm = llm, retriever= retriever, memory = memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is the authors expectation to this paper?',\n",
       " 'chat_history': [HumanMessage(content='What is the paper all about?'),\n",
       "  AIMessage(content='The paper discusses the quantization of a large language model called Mistral-7B-v0.1. The quantization process makes the language model accessible for smaller devices that rely on CPU-based processing. The paper explores the potential of using the quantized model for processing text information within PDFs. It also mentions the need for improving the PDF parsing and chunking process and fine-tuning the model for specific use cases before quantization. The paper concludes by highlighting the promising beginning of a locally hosted model operating exclusively on CPU and RAM.'),\n",
       "  HumanMessage(content='What is Llama.cpp?'),\n",
       "  AIMessage(content='Llama.cpp refers to an open-source library that is designed to run large language models like Llama and Alpaca on commodity hardware. It provides options for optimizations to make the models smaller and faster. Llama.cpp uses normal quantization instead of Low-Rank Adaption (LoRA) or Quantized Low-Rank Adaption (QloRA).'),\n",
       "  HumanMessage(content='What is the authors expection to this paper?'),\n",
       "  AIMessage(content='The authors expect that the quantization of the Mistral-7B-v0.1 model will make large language models accessible for smaller devices, particularly those relying on CPU-based processing. They believe that this model can be deployed as a locally hosted assistant for internal data handling, providing complete security through the use of open-source tools. The authors also anticipate the deployment of increasingly accessible language models on standard hardware configurations as the trend toward downsizing LLMs continues. Additionally, they mention the potential for improving the PDF parsing and chunking process to address the issue of irrelevant information in some test segments. The authors also mention the need to fine-tune the model for specific use cases before quantization to improve performance.'),\n",
       "  HumanMessage(content='What is the authors expectation to this paper?'),\n",
       "  AIMessage(content=\"The authors' expectations for this paper are to demonstrate the potential of quantized language models, specifically Mistral-7B-v0.1, in making large language models accessible for smaller devices relying on CPU-based processing. They anticipate that this model can serve as a locally hosted assistant for internal data handling, ensuring complete security. The authors mention the deployment of increasingly accessible language models on standard hardware configurations as the trend toward downsizing language models continues. They also acknowledge the need for further improvements, such as fine-tuning the model for specific use-cases before quantization and improving the PDF parsing and chunking process.\")],\n",
       " 'answer': \"The authors' expectations for this paper are to demonstrate the potential of quantized language models, specifically Mistral-7B-v0.1, in making large language models accessible for smaller devices relying on CPU-based processing. They anticipate that this model can serve as a locally hosted assistant for internal data handling, ensuring complete security. The authors mention the deployment of increasingly accessible language models on standard hardware configurations as the trend toward downsizing language models continues. They also acknowledge the need for further improvements, such as fine-tuning the model for specific use-cases before quantization and improving the PDF parsing and chunking process.\"}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the authors expectation to this paper?\"\n",
    "chain.invoke({'question': query})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
